{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7341b4-aa5f-4ad3-ae04-5ee2340c2a2e",
   "metadata": {},
   "source": [
    "# Training with Distance Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7eab6c-fc4c-4932-8fb3-85a177852905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# created library\n",
    "import dataaugmentation\n",
    "import mydata\n",
    "from skindetection import SkinExtraction\n",
    "from skincolors import IndividualTypologyAngle\n",
    "from performance import PerformanceMeasure, PerformanceEstimation\n",
    "from distance import DistanceMeasure\n",
    "\n",
    "from mymodels import TransDataset, EfficientB3Model\n",
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803497df-ac13-4bf6-845c-b54e7238fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"UTKFace\"\n",
    "root = f\"../../dataset/AAAI 2025/{db}/\"\n",
    "polynomial_save_file = f\"{root}model/pickle/polynomial_model_ef3.pkl\"\n",
    "\n",
    "# Load model\n",
    "with open(polynomial_save_file, \"rb\") as f:\n",
    "    polynomial_model = pickle.load(f)\n",
    "\n",
    "BASELINE_FILE = \"44_0_3_20170119195216221.jpg.chip.jpg\"\n",
    "STANDARD = 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d15efa3-a357-44b5-ada6-3ac665d67bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9640472, 0.9678538])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_model(np.array([45, 50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d707ad-e161-45bb-b0d8-0f382fa8f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional ITA values: 17.321600446516427\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(f\"{root}mask/{BASELINE_FILE}\")\n",
    "baseline_skin_pixels_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "ita = IndividualTypologyAngle(baseline_skin_pixels_image)\n",
    "baseline_mean_ita = ita.get_mean_ita()\n",
    "print(f\"Conventional ITA values: {baseline_mean_ita}\")\n",
    "baseline_nuance_ita = ita.get_nuance_ita()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b413c62-faed-482f-bd61-d4f1f0102f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = mydata.load_and_process_csv(f\"{root}/dataframe/df_train.csv\")\n",
    "df_valid = mydata.load_and_process_csv(f\"{root}/dataframe/df_valid.csv\")\n",
    "df_test = mydata.load_and_process_csv(f\"{root}/dataframe/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a5c5a1-c5fc-4769-96c4-c4b6f1b66b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_penalty(distances):\n",
    "    diff = polynomial_model(np.array(distances)) - STANDARD\n",
    "    diff = torch.tensor(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e687acd8-d401-4262-827b-74b17044f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H*W:  (200, 200)\n"
     ]
    }
   ],
   "source": [
    "ycol=\"labels\"\n",
    "batch_size = 8\n",
    "train_loader, valid_loader, test_loader = training.create_dataloaders(df_train, df_valid, df_test, ycol, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50f3c36-0b29-48dc-976e-0f405888df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBCEWithLogitsLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets, distances, device): # criterion\n",
    "        bce_loss = F.binary_cross_entropy(outputs, targets)\n",
    "        penalty = calculate_distance_penalty(distances).to(device)\n",
    "        return bce_loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1377ca-7a70-4847-bc51-1d650a58ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, num_epochs=25, lr=1e-5):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_f1s = []\n",
    "    valid_f1s = []\n",
    "    train_aucs = []\n",
    "    valid_aucs = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    criterion = CustomBCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for inputs, labels, masks in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # data -> GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            distances = []\n",
    "            #print(\"masks:\", masks.shape)\n",
    "            for mask in masks:\n",
    "                mask_np = mask.cpu().numpy()\n",
    "                mask_np = np.transpose(mask_np, (1, 2, 0)) \n",
    "                mask_np = mask_np * 255\n",
    "                mask_np = mask_np.astype(np.uint8)\n",
    "\n",
    "                #print(\"mask_np max:\", mask_np.max())\n",
    "                #print(\"mask_np shape:\", mask_np.shape)\n",
    "                \n",
    "                ita = IndividualTypologyAngle(mask_np)\n",
    "                mask_nuance_ita = ita.get_nuance_ita()\n",
    "                dm = DistanceMeasure(baseline_nuance_ita, mask_nuance_ita)\n",
    "                distance = distance = dm.sign_wasserstein_distance()\n",
    "                distances.append(distance)\n",
    "                \n",
    "            print(\"distances: \", distances[0])\n",
    "            loss = criterion(outputs.squeeze(), labels, distances, device)\n",
    "            loss.backward() # Partial Derivative\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            all_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_f1 = f1_score(all_labels, [1 if x >= 0 else 0 for x in all_preds])\n",
    "        epoch_auc = roc_auc_score(all_labels, all_preds)\n",
    "        epoch_acc = accuracy_score(all_labels, [1 if x >= 0 else 0 for x in all_preds])\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_f1s.append(epoch_f1)\n",
    "        train_aucs.append(epoch_auc)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        #print(f'Epoch {epoch}/{num_epochs - 1} | Loss: {epoch_loss:.4f} | F1: {epoch_f1:.4f} | AUC: {epoch_auc:.4f}')\n",
    "        \n",
    "        model.eval() # Validation だから。\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # data -> GPU\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        val_f1 = f1_score(val_labels, [1 if x >= 0.5 else 0 for x in val_preds])\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_acc = accuracy_score(val_labels, [1 if x >= 0.5 else 0 for x in val_preds])\n",
    "\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_f1s.append(val_f1)\n",
    "        valid_aucs.append(val_auc)\n",
    "        valid_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Validation Accuracy: {val_acc:.4f} | Loss: {val_loss:.4f} | F1: {val_f1:.4f} | AUC: {val_auc:.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    epochs = range(num_epochs)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, valid_losses, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, valid_accuracies, label='Valid Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, train_f1s, label='Train F1 Score')\n",
    "    plt.plot(epochs, valid_f1s, label='Valid F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2046c335-3ffc-4ef5-a419-5b7b88715659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances:  31.92417859646297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      5\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_model(model, train_loader, valid_loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     44\u001b[0m     distances\u001b[38;5;241m.\u001b[39mappend(distance)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances: \u001b[39m\u001b[38;5;124m\"\u001b[39m, distances[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels, distances, device)\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Partial Derivative\u001b[39;00m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mCustomBCEWithLogitsLoss.forward\u001b[0;34m(self, outputs, targets, distances, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, targets, distances, device): \u001b[38;5;66;03m# criterion\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     bce_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(outputs, targets)\n\u001b[0;32m---> 15\u001b[0m     penalty \u001b[38;5;241m=\u001b[39m calculate_distance_penalty(distances)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bce_loss \u001b[38;5;241m+\u001b[39m penalty\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientB3Model().to(device) # model -> GPU\n",
    "\n",
    "num_epochs = 20\n",
    "lr = 0.0001\n",
    "train_model(model, train_loader, valid_loader, num_epochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e44e8-ae05-4db1-becc-7e6e5ff8899e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
